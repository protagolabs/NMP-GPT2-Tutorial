# -*- coding: utf-8 -*-
"""story_hf_trainer_model_parallel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VZFUlzIqd7lboJ1kGbTc3zqmo0zBB5pe

## Introduction:


## Some infomation about this task:

1. Story generation: We will use the GPT-2 to train a model which can generate some stories.
2. Dataset: We will use the "KATANABRAVE/stories" dataset from HuggingFace
3. [GPT model](https://huggingface.co/docs/transformers/v4.32.0/en/model_doc/gpt2#transformers.GPT2Model), we will use the HuggingFace implementation

Ensure you have install the correct libraries before running this code.
Required packages:
numpy pandas torch torchvision torch-optimizer tqdm accelerate transformers matplotlib datasets huggingface-hub sentencepiece argparse tensorboard
If your modified code includes other additional libraries, please add them to the line.
"""


"""### Step 1: Define the training parameters"""

# HuggingFace Trainer
import transformers
from transformers import TrainingArguments

# Here we want to close the wandb, if we use the huggingface's tranier.
import os
from dataclasses import dataclass, field


@dataclass
class ModelTrainingArguments(TrainingArguments):
    model_name_or_path: str = field(
        default='gpt2',
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models","required": False}
    )
    # model_name_or_path: str = 'gpt2'
    data: str = "KATANABRAVE/stories"

    seed: int = 32
    evaluation_strategy: str = "epoch"
    learning_rate: int = 2e-4
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 16
    weight_decay: float = 0.01
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1
    logging_steps: int = 100
    save_total_limit: int = 3
    warmup_steps: int = 200
    num_train_epochs: int = 1000
    max_steps: int = 1000
    save_steps: int = 500
    fp16: bool = False
    report_to: str = "none"
    output_dir: str = "saved_model"

training_args = ModelTrainingArguments()

"""### Step 2: Load the model and tokenizer"""

from transformers import GPT2Tokenizer, AutoModelForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained(training_args.model_name_or_path)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model = AutoModelForCausalLM.from_pretrained(training_args.model_name_or_path,device_map='balanced')
model.train()
"""
Model Parallel Command
"""
model.parallelize()

"""### Step 3: Prepare the dataset.

"""

from datasets import load_dataset
from transformers import DataCollator

# named the dataset path as training_args.data
# Import the dataset, which is a demo for some D&D stories.
dataset = load_dataset(training_args.data)

"""### Step 4: Define the optimizer and scheduler."""

from transformers import get_linear_schedule_with_warmup, AdamW

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': training_args.weight_decay},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]
optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)

schedule_total = training_args.max_steps

scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=schedule_total
)

"""## Netmind-Part

### Step 5: Initialize the Netmind nmp
"""

from NetmindMixins.Netmind import nmp
"""
We turn off the ddp function for model parallel.
"""
nmp.init(use_ddp=False)

"""### Step 6: Define the NetmindTrainerCallback

We will use it in the trainer initialize
"""

import transformers
from NetmindMixins.Netmind import NetmindTrainerCallback

class CustomTrainerCallback(NetmindTrainerCallback):
    def __init__(self):
        super().__init__()

    '''
    Add custom training metrics
    '''

    def on_step_end(self, args: transformers.TrainingArguments, state: transformers.TrainerState,
                    control: transformers.TrainerControl, **kwargs):
        kwargs["custom_metrics"] = {}
        return super().on_step_end(args, state, control, **kwargs)

    '''
    Add custom evaluation metrics
    '''

    def on_evaluate(self, args: transformers.TrainingArguments, state: transformers.TrainerState,
                    control: transformers.TrainerControl, **kwargs):
        kwargs["custom_metrics"] = {}
        return super().on_evaluate(args, state, control, **kwargs)

"""### Setp 7: Start Training"""

from transformers import Trainer

nmp.init_train_bar(max_steps=training_args.max_steps)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    optimizers=(optimizer, scheduler),
    callbacks=[CustomTrainerCallback]
)
trainer.remove_callback(transformers.trainer_callback.PrinterCallback)
trainer.remove_callback(transformers.trainer_callback.ProgressCallback)

trainer.train()

nmp.finish_training()  # Finish the training. It should be placed at the end of file

